import cv2
import numpy as np 
face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
def face_extractor(img):
    
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_classifier.detectMultiScale(gray, 1.3, 5)
    
    if faces is ():
        return None
    
    #crop all face found
    
    for (x,y,w,h) in faces:
        cropped_face = img[y:y+h, x:x+w]
        
    return cropped_face

cap = cv2.VideoCapture(0)
count = 0

# collect 1000 samples of yr face from webcam

while True:
    
    ret, frame = cap.read()
    if face_extractor(frame) is not None:
        count += 1
        face = cv2.resize(face_extractor(frame), (200,200))
        #face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)
        
 #save file in specified dir with unique name
        
        file_name_path = './faces/user/' + str(count) + '.jpg'
        cv2.imwrite(file_name_path, face)
        
 #put count on imgs & disp live count
        
        cv2.putText(face, str(count), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)
        cv2.imshow('Face Cropper', face)
        
    else:
        print("Face not found")
        pass
    
    if cv2.waitKey(1) == 13 or count == 1000:
        break
        
cap.release()
cv2.destroyAllWindows()

print("done")

# Second Model Trained=================

import cv2
import numpy as np
from os import listdir

from os.path import isfile, join

data_path = './faces/user1/'
onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))]

# create arrays for training data and labels

Training_Data1, Labels1 = [], []

#================open training img in our datapath================================
#==========create a numpy array for traning data==========================

for i, files in enumerate(onlyfiles):
                               image_path = data_path + onlyfiles[i]
                               images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
                               Training_Data1.append(np.asarray(images, dtype = np.uint8))
                               Labels1.append(1)
    
#==========create numpy array for both training data and labels==========================
Labels1 = np.asarray(Labels1, dtype= np.int32)

sonam_model1 = cv2.face_LBPHFaceRecognizer.create()

#=================let's train our model=============================
sonam_model1.train(np.asarray(Training_Data1), np.asarray(Labels1))

print("Model trained")

# Second Face Samples======

def face_extractor(img):
    
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_classifier.detectMultiScale(gray, 1.3, 5)
    
    if faces is ():
        return None
    
    #crop all face found
    
    for (x,y,w,h) in faces:
        cropped_face = img[y:y+h, x:x+w]
        
    return cropped_face

cap = cv2.VideoCapture(0)
count = 0

# collect 1000 samples of yr face from webcam

while True:
    
    ret, frame = cap.read()
    if face_extractor(frame) is not None:
        count += 1
        face = cv2.resize(face_extractor(frame), (200,200))
        #face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)
        
 #save file in specified dir with unique name
        
        file_name_path = './faces/user1/' + str(count) + '.jpg'
        cv2.imwrite(file_name_path, face)
        
 #put count on imgs & disp live count
        
        cv2.putText(face, str(count), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)
        cv2.imshow('Face Cropper', face)
        
    else:
        print("Face not found")
        pass
    
    if cv2.waitKey(1) == 13 or count == 1000:
        break
        
cap.release()
cv2.destroyAllWindows()

print("done")

#  When recognises sec face , it can be yr friends or family member...! It will launch an instance , Create an EBS volume and attached it to the instance.

import cv2
import numpy
from subprocess import getoutput
import time
import asyncio
import pywhatkit as kit
from gmail import GMail,Message,gmail 

face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

def face_detector(img, size=0.5):
    
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_classifier.detectMultiScale(gray, 1.3, 5)
    
    if faces is ():
        return img, []
    
    #crop all face found
    for (x,y,w,h) in faces:
        cv2.rectangle(img, (x,y), (x+w, y+h), (0,255,255),2)
        r = img[y:y+h, x:x+w]
        r = cv2.resize(r, (200,200))
    return img, r

def infra_launch():
            instance_id = getoutput('aws ec2 run-instances --image-id ami-0ad704c126371a549 --instance-type t2.micro --count 1 --subnet-id subnet-c77064af --security-group-ids sg-0aa265c2c7073e28c --tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=Summer_instance_sonam_6}]" --key-name sprog_21 --query "Instances[0].InstanceId"')
            time.sleep(10)
            instance_id=instance_id[1:-1]
            print(instance_id)
            volume_id = getoutput('aws ec2 create-volume --volume-type gp2 --size 4 --availability-zone ap-south-1a --tag-specifications "ResourceType=volume,Tags=[{Key=Name,Value=Summer_volume_sonam_6}]" --query "VolumeId"')
            time.sleep(2)
            success = getoutput(f'aws ec2 attach-volume --volume-id {volume_id[1:-1]} --instance-id {instance_id} --device /dev/sdf')
            print(success)
            print("AWS setup done")
       
       
        # open webcam
        
flag1=0
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    image, face = face_detector(frame)
    try:
        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)
        
        # pass face to prediction model
        #"results" comprises of tuple containing label and confidence value
        results = sonam_model.predict(face)
        results1 = sonam_model1.predict(face)
  
        
        if results[1] < 500:
            confidence = int( 100 * (1 - (results[1])/400))
            display_string = str(confidence) + '% Confident it is User'
            
            cv2.putText(image, display_string, (100, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255,120,150), 2) 
        
            if confidence > 90:
                cv2.putText(image, "HLO SONAM", (240, 440), cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,0), 2)
                cv2.imshow('Face Recognition', image)
                
               
                if flag1 == 0:
                    infra_launch()
                    print(flag1)
                    flag1+=1
                    
        if results1[1] < 500:
            confidence1 = int( 100 * (1 - (results1[1])/400))
            display_string = str(confidence1) + '% Confident it is User2'
            
            cv2.putText(image, display_string, (100, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255,120,150), 2) 
        
            if confidence1 > 85:
                cv2.putText(image, "HLO GAGAN JI", (240, 440), cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,0), 2)
                cv2.imshow('Face Recognition', image)

       

        else:
            cv2.putText(image, "I DON'T KNOW", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2 )
            cv2.imshow('Face Recognition', image )
            
    except:
        cv2.putText(image, "No Face Found", (220, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)
        cv2.putText(image, "looking for face", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)
        cv2.imshow('Face Recognition', image )
        pass
    
    if cv2.waitKey(1) == 13:
        break
        
cap.release()
cv2.destroyAllWindows()


        
        
              
            
            
            
